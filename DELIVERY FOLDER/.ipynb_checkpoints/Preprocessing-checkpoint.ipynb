{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH THIS NOTEBOOK WE ARE ADDING NEW FEATURES TO THE DATASETS\n",
    "\n",
    "PREPROCESSING ON DATA TO GET THEM READY FOR THE MODELLING PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/x_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2689, 1027, 2696, 2698, 1035,  144, 2704, 1554, 2705, 2707, 2711,\n",
       "       2712, 1051, 2718,  546, 1058,  549, 1065,  554,  686,  688, 1206,\n",
       "       2360, 2365, 1472, 1600, 1603, 1732, 1608, 2249, 1356, 1618, 1365,\n",
       "       1371, 2396, 1633, 2401, 2410, 1516, 2678, 2681, 2682, 2683])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sku.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1027, 1035,  144, 1051,  546, 1058,  549, 1065,  554,  686,  688,\n",
       "       1206])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sku.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1027, 1035,  144, 2704, 1051, 2718,  546, 1058,  549, 1065,  554,\n",
       "        686,  688, 1206, 1603, 1732, 2249, 1356, 1371, 2396, 1633, 2401,\n",
       "       2410, 1516])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop 2689 becaouse it is not correlated to anything\n",
    "#drop not stationary sku\n",
    "stationary = [1356,1371,1516,1603,1633,1732,2249,2396,2401,2410,2704,2718,1027,\n",
    "              1035,144,1051,546,1058,549,1065,554,686,688,1206]\n",
    "train_df = train_df[train_df[\"sku\"].isin(stationary)]\n",
    "train_df.sku.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.rename({\"Unnamed: 0\":\"date\"}, axis=1)\n",
    "train_df.date = train_df.date.apply(lambda x:datetime.strptime(x[3:], '%d %B %Y'))\n",
    "test_df = test_df.rename({\"Unnamed: 0\":\"date\"}, axis=1)\n",
    "test_df.date = test_df.date.apply(lambda x:datetime.strptime(x[3:], '%d %B %Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3168 entries, 134 to 5186\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   date                 3168 non-null   datetime64[ns]\n",
      " 1   sku                  3168 non-null   int64         \n",
      " 2   pack                 3168 non-null   object        \n",
      " 3   size (GM)            3168 non-null   float64       \n",
      " 4   brand                3168 non-null   object        \n",
      " 5   price                3168 non-null   float64       \n",
      " 6   POS_exposed w-1      3168 non-null   float64       \n",
      " 7   volume_on_promo w-1  3168 non-null   float64       \n",
      " 8   sales w-1            3168 non-null   float64       \n",
      " 9   scope                3168 non-null   int64         \n",
      " 10  target               3168 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(6), int64(2), object(2)\n",
      "memory usage: 297.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# on train dataframe values of \"2016-12-10\" raw regarding the week -1 are null so we drop them\n",
    "train_df = train_df.dropna()\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisogna decidere se lasciare l'ultima riga del test con target nan (perchè andrà predetto per la consegna) o se togliere la riga (perchè non si riesce a calcolarci il mape non avendo il valore reale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make of target column on test set\n",
    "temp = pd.DataFrame(columns = train_df.columns)\n",
    "\n",
    "for sku in test_df['sku'].unique():\n",
    "    sales_sku = test_df.loc[test_df['sku'] == sku]\n",
    "    #nan on the last row becouse we don't have the value\n",
    "    sales_sku['target'] = sales_sku[\"sales w-1\"].shift(-1)\n",
    "    temp = pd.concat((temp, sales_sku), axis = 0)\n",
    "\n",
    "test_df = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "def scale(feature, scaler = min_max_scaler):\n",
    "    size = len(feature)\n",
    "    return scaler.fit_transform(np.array([feature]).reshape(size, 1)).T[0]\n",
    "def unscale(scaled, original, scaler = min_max_scaler):\n",
    "    size2 = len(scaled)\n",
    "    size1 = len(original)\n",
    "    return scaler.fit(np.array([original]).reshape(size1, 1)).inverse_transform(np.array([scaled]).reshape(size2, 1)).T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \n",
    "    df2 = pd.DataFrame(columns= df.columns)\n",
    "    sku_groups = {\n",
    "    \"group_A\" : [688, 1058, 549, 546, 1027, 554, 1035, 1206, 1065],\n",
    "    \"group_B\" : [144, 686, 1051],\n",
    "    \"group_C\" : [2249, 2401, 2410],\n",
    "    \"group_D\" : [1356, 1371, 2704],\n",
    "    \"group_E\" : [1516, 1633],  \n",
    "    \"Not_grouped\" : [2718, 1603, 1732, 2396]\n",
    "    }\n",
    "    \n",
    "\n",
    "    for sku in df['sku'].unique():\n",
    "        sales_sku = df.loc[df['sku'] == sku]\n",
    "        \n",
    "        for l,g in sku_groups.items():\n",
    "            if sku in g:\n",
    "                if \"corr_group\" in sales_sku.columns:\n",
    "                    sales_sku[\"corr_group\"] = sales_sku[\"corr_group\"].apply(lambda x: x + \" \" + l)\n",
    "                else:\n",
    "                    sales_sku[\"corr_group\"] = np.full((len(sales_sku[\"price\"]),), l)\n",
    "              \n",
    "        sales_sku['sales w-2'] = sales_sku['sales w-1'].shift(1)\n",
    "        sales_sku['sales w-3'] = sales_sku['sales w-1'].shift(2)\n",
    "                    \n",
    "        sales_sku[\"rolling1\"] = sales_sku[\"sales w-1\"].rolling(1).mean()\n",
    "        sales_sku[\"rolling2\"] = sales_sku[\"sales w-1\"].rolling(2).mean()\n",
    "        sales_sku[\"rolling3\"] = sales_sku[\"sales w-1\"].rolling(3).mean()\n",
    "        sales_sku[\"rolling4\"] = sales_sku[\"sales w-1\"].rolling(4).mean()\n",
    "        sales_sku[\"rolling5\"] = sales_sku[\"sales w-1\"].rolling(5).mean()\n",
    "\n",
    "        sales_sku['diff1'] = -(sales_sku['sales w-1'] - sales_sku['sales w-1'].shift(1))\n",
    "        sales_sku['diff2'] = -(sales_sku['sales w-1'] - sales_sku['sales w-1'].shift(2))\n",
    "        sales_sku['diff3'] = -(sales_sku['sales w-1'] - sales_sku['sales w-1'].shift(3))\n",
    " \n",
    "        sales_sku[\"scaled_target\"] = scale(sales_sku[\"target\"])\n",
    "        sales_sku[\"scaled_price\"] = scale(sales_sku[\"price\"])\n",
    "        sales_sku['scaled_sales1'] = scale(sales_sku['sales w-1'])\n",
    "        sales_sku['scaled_sales2'] = sales_sku['scaled_sales1'].shift(1)\n",
    "        sales_sku['scaled_sales3'] = sales_sku['scaled_sales1'].shift(2)\n",
    "        sales_sku['scaled_sales4'] = sales_sku['scaled_sales1'].shift(3)\n",
    "        sales_sku['scaled_sales8'] = sales_sku['scaled_sales1'].shift(7)\n",
    "\n",
    "        sales_sku[\"scaled_promo\"] = scale(sales_sku[\"volume_on_promo w-1\"])\n",
    "        \n",
    "        sales_sku[\"scaled_rolling1\"] = sales_sku[\"scaled_sales1\"].rolling(1).mean()\n",
    "        sales_sku[\"scaled_rolling2\"] = sales_sku[\"scaled_sales1\"].rolling(2).mean()\n",
    "        sales_sku[\"scaled_rolling3\"] = sales_sku[\"scaled_sales1\"].rolling(3).mean()\n",
    "        sales_sku[\"scaled_rolling4\"] = sales_sku[\"scaled_sales1\"].rolling(4).mean()\n",
    "        sales_sku[\"scaled_rolling5\"] = sales_sku[\"scaled_sales1\"].rolling(5).mean()\n",
    "        \n",
    "        sales_sku['scaled_diff1'] = -(sales_sku['scaled_sales1'] - sales_sku['scaled_sales1'].shift(1))\n",
    "        sales_sku['scaled_diff2'] = -(sales_sku['scaled_sales1'] - sales_sku['scaled_sales1'].shift(2))\n",
    "        sales_sku['scaled_diff3'] = -(sales_sku['scaled_sales1'] - sales_sku['scaled_sales1'].shift(3))\n",
    "        sales_sku['percentage_diff1'] = -(sales_sku['sales w-1'] - sales_sku['sales w-1'].shift(1))/sales_sku['sales w-1']\n",
    "        sales_sku['scaled_price_diff1'] = -(sales_sku['scaled_price'] - sales_sku['scaled_price'].shift(1))\n",
    "      \n",
    "        df2 = pd.concat((df2, sales_sku), axis = 0)\n",
    "    \n",
    "    #df2 = df2.dropna()\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    onehot_encoded = df2[\"corr_group\"].str.get_dummies(\" \") > 0\n",
    "    onehot = pd.DataFrame(onehot_encoded, index = df2.index)\n",
    "    \n",
    "    onehot_encoded = df2[\"brand\"].str.get_dummies() > 0\n",
    "    onehot2 = pd.DataFrame(onehot_encoded, index = df2.index)\n",
    "\n",
    "    df = pd.concat([df2, onehot, onehot2], axis=1, sort=False)\n",
    "    \n",
    "    df = df.drop(columns=[\"pack\",\"size (GM)\",\"brand\",\"corr_group\",\"POS_exposed w-1\"])\n",
    "    \n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['dayofmonth'] = df['date'].dt.day\n",
    "    df['weekofyear'] = df['date'].dt.weekofyear\n",
    "    \n",
    "    df['scaled_quarter'] = scale(df['quarter'])\n",
    "    df['scaled_month'] = scale(df['month'])\n",
    "    df['scaled_year'] = scale(df['year'])\n",
    "    df['scaled_dayofyear'] = scale(df['dayofyear'])\n",
    "    df['scaled_dayofmonth'] = scale(df['dayofmonth'])\n",
    "    df['scaled_weekofyear'] = scale(df['weekofyear'])\n",
    "    \n",
    "    df[\"sku\"] = df[\"sku\"].apply(lambda x: int(x))\n",
    "    \n",
    "    cols = list(df.columns.values)\n",
    "    cols.pop(cols.index('target'))\n",
    "    cols.pop(cols.index('scaled_target'))\n",
    "    df = df[cols+['scaled_target','target']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"scope\"] = train_df.scope.apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_scope0 = train_df.loc[train_df.scope == 0]\n",
    "train_df_scope1 = train_df.loc[train_df.scope == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df1 = pd.concat([train_df_scope1,test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_scope0 = create_features(train_df_scope0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df1 = create_features(all_df1)\n",
    "all_df1 = all_df1.set_index(\"date\")\n",
    "processed_train_scope1 = all_df1[:\"2019-06-22\"].reset_index()\n",
    "processed_test = all_df1[\"2019-06-23\":].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3168 entries, 799 to 1583\n",
      "Data columns (total 56 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   date                 3168 non-null   datetime64[ns]\n",
      " 1   sku                  3168 non-null   int64         \n",
      " 2   price                3168 non-null   float64       \n",
      " 3   volume_on_promo w-1  3168 non-null   float64       \n",
      " 4   sales w-1            3168 non-null   float64       \n",
      " 5   scope                3168 non-null   object        \n",
      " 6   sales w-2            3144 non-null   float64       \n",
      " 7   sales w-3            3120 non-null   float64       \n",
      " 8   rolling1             3168 non-null   float64       \n",
      " 9   rolling2             3144 non-null   float64       \n",
      " 10  rolling3             3120 non-null   float64       \n",
      " 11  rolling4             3096 non-null   float64       \n",
      " 12  rolling5             3072 non-null   float64       \n",
      " 13  diff1                3144 non-null   float64       \n",
      " 14  diff2                3120 non-null   float64       \n",
      " 15  diff3                3096 non-null   float64       \n",
      " 16  scaled_price         3168 non-null   float64       \n",
      " 17  scaled_sales1        3168 non-null   float64       \n",
      " 18  scaled_sales2        3144 non-null   float64       \n",
      " 19  scaled_sales3        3120 non-null   float64       \n",
      " 20  scaled_promo         3168 non-null   float64       \n",
      " 21  scaled_rolling1      3168 non-null   float64       \n",
      " 22  scaled_rolling2      3144 non-null   float64       \n",
      " 23  scaled_rolling3      3120 non-null   float64       \n",
      " 24  scaled_rolling4      3096 non-null   float64       \n",
      " 25  scaled_rolling5      3072 non-null   float64       \n",
      " 26  scaled_diff1         3144 non-null   float64       \n",
      " 27  scaled_diff2         3120 non-null   float64       \n",
      " 28  scaled_diff3         3096 non-null   float64       \n",
      " 29  percentage_diff1     3144 non-null   float64       \n",
      " 30  scaled_price_diff1   3144 non-null   float64       \n",
      " 31  Not_grouped          1584 non-null   object        \n",
      " 32  group_C              1584 non-null   object        \n",
      " 33  group_D              1584 non-null   object        \n",
      " 34  group_E              1584 non-null   object        \n",
      " 35  BRAND1               1584 non-null   object        \n",
      " 36  BRAND3               1584 non-null   object        \n",
      " 37  BRAND5               1584 non-null   object        \n",
      " 38  quarter              3168 non-null   int64         \n",
      " 39  month                3168 non-null   int64         \n",
      " 40  year                 3168 non-null   int64         \n",
      " 41  dayofyear            3168 non-null   int64         \n",
      " 42  dayofmonth           3168 non-null   int64         \n",
      " 43  weekofyear           3168 non-null   int64         \n",
      " 44  scaled_quarter       3168 non-null   float64       \n",
      " 45  scaled_month         3168 non-null   float64       \n",
      " 46  scaled_year          3168 non-null   float64       \n",
      " 47  scaled_dayofyear     3168 non-null   float64       \n",
      " 48  scaled_dayofmonth    3168 non-null   float64       \n",
      " 49  scaled_weekofyear    3168 non-null   float64       \n",
      " 50  scaled_target        3168 non-null   float64       \n",
      " 51  target               3168 non-null   float64       \n",
      " 52  group_A              1584 non-null   object        \n",
      " 53  group_B              1584 non-null   object        \n",
      " 54  BRAND2               1584 non-null   object        \n",
      " 55  BRAND4               1584 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(36), int64(7), object(12)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "processed_train = pd.concat([processed_train_scope0,processed_train_scope1])\n",
    "processed_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train[\"BRAND1\"] = processed_train[\"BRAND1\"].fillna(False)\n",
    "processed_train[\"BRAND2\"] = processed_train[\"BRAND2\"].fillna(False)\n",
    "processed_train[\"BRAND3\"] = processed_train[\"BRAND3\"].fillna(False)\n",
    "processed_train[\"BRAND4\"] = processed_train[\"BRAND4\"].fillna(False)\n",
    "processed_train[\"BRAND5\"] = processed_train[\"BRAND5\"].fillna(False)\n",
    "processed_train[\"group_A\"] = processed_train[\"group_A\"].fillna(False)\n",
    "processed_train[\"group_B\"] = processed_train[\"group_B\"].fillna(False)\n",
    "processed_train[\"group_C\"] = processed_train[\"group_C\"].fillna(False)\n",
    "processed_train[\"group_D\"] = processed_train[\"group_D\"].fillna(False)\n",
    "processed_train[\"group_E\"] = processed_train[\"group_E\"].fillna(False)\n",
    "processed_train[\"Not_grouped\"] = processed_train[\"Not_grouped\"].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'sku', 'price', 'volume_on_promo w-1', 'sales w-1', 'scope',\n",
       "       'sales w-2', 'sales w-3', 'rolling1', 'rolling2', 'rolling3',\n",
       "       'rolling4', 'rolling5', 'diff1', 'diff2', 'diff3', 'scaled_price',\n",
       "       'scaled_sales1', 'scaled_sales2', 'scaled_sales3', 'scaled_promo',\n",
       "       'scaled_rolling1', 'scaled_rolling2', 'scaled_rolling3',\n",
       "       'scaled_rolling4', 'scaled_rolling5', 'scaled_diff1', 'scaled_diff2',\n",
       "       'scaled_diff3', 'percentage_diff1', 'scaled_price_diff1', 'group_A',\n",
       "       'group_B', 'BRAND2', 'BRAND4', 'quarter', 'month', 'year', 'dayofyear',\n",
       "       'dayofmonth', 'weekofyear', 'scaled_quarter', 'scaled_month',\n",
       "       'scaled_year', 'scaled_dayofyear', 'scaled_dayofmonth',\n",
       "       'scaled_weekofyear', 'scaled_target', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train.to_csv(\"processed_train.csv\")\n",
    "processed_test.to_csv(\"processed_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

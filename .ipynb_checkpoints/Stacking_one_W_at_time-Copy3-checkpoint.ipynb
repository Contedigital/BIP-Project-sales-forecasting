{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime, time, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "%matplotlib inline\n",
    "\n",
    "#models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    " \n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "\n",
    "figsize=(12,9)\n",
    "\n",
    "np.random.seed(238746)\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN-TEST SPLIT DATE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SplitTestDate = \"2019-06-23\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL SELECTION AND FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"xgb\" : xgb.XGBRegressor(n_estimators=500, learning_rate=0.1),\n",
    "    \"extra\": ExtraTreesRegressor(n_estimators=500),\n",
    "    #\"forest\" : RandomForestRegressor(n_estimators=100)\n",
    "}\n",
    "\n",
    "features_for_model = {\n",
    "\n",
    "    \"xgb\" : ['scaled_month',\n",
    "            \"group_A\",\"group_B\",\"BRAND2\",\"BRAND4\",\n",
    "           'scaled_dayofyear','scaled_dayofmonth','scaled_weekofyear',\"scaled_sales1\",\"scaled_sales2\",\"scaled_sales3\",\n",
    "            'scaled_promo','scaled_diff1','scaled_diff2','percentage_diff1','scaled_price',\"scaled_rolling2\"],\n",
    "    \"extra\": ['scaled_month',\n",
    "            \"group_A\",\"group_B\",\"BRAND2\",\"BRAND4\",\n",
    "           'scaled_dayofyear','scaled_dayofmonth','scaled_weekofyear',\"scaled_sales1\",\"scaled_sales2\",\"scaled_sales3\",\n",
    "            'scaled_promo','scaled_diff1','scaled_diff2','percentage_diff1','scaled_price',\"scaled_rolling2\"],\n",
    "    \"forest\": ['scaled_month',\n",
    "            \"group_A\",\"group_B\",\"BRAND2\",\"BRAND4\",\n",
    "           'scaled_dayofyear','scaled_dayofmonth','scaled_weekofyear',\"scaled_sales1\",\"scaled_sales2\",\"scaled_sales3\",\n",
    "            'scaled_promo','scaled_diff1','scaled_diff2','percentage_diff1','scaled_price',\"scaled_rolling2\"],\n",
    "    \n",
    "    \"final\": [1027, 1035,  144, 1051,  546, 1058,  549, 1065,  554,  686,  688,\n",
    "       1206,\"group_A\",\"group_B\",\"xgb\",\"extra\"]\n",
    " \n",
    "}\n",
    "\n",
    "finalreg = RandomForestRegressor(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCALING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "def scale(feature, scaler = min_max_scaler):\n",
    "    size = len(feature)\n",
    "    return scaler.fit_transform(np.array([feature]).reshape(size, 1)).T[0]\n",
    "def unscale(scaled, original, scaler = min_max_scaler):\n",
    "    size2 = len(scaled)\n",
    "    size1 = len(original)\n",
    "    return scaler.fit(np.array([original]).reshape(size1, 1)).inverse_transform(np.array([scaled]).reshape(size2, 1)).T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAPE AND PLOT FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def plot_results(ytrue, prediction, label):\n",
    "    df_ytrue = pd.DataFrame(ytrue)\n",
    "    df_ytrue['predicted'] =  prediction \n",
    "    plt.plot(df_ytrue[label],label='data')\n",
    "    plt.plot(df_ytrue['predicted'],label='predicted')\n",
    "    plt.xticks(rotation=45)\n",
    "    mape = mean_absolute_percentage_error(df_ytrue[label],df_ytrue['predicted'])\n",
    "\n",
    "    plt.title(\"Forecasting on Test Set MAPE=%.3f\"%mape)\n",
    "    plt.legend();\n",
    "    plt.show()\n",
    "    return mape\n",
    "\n",
    "def scaled_mape(prediction, y_test, rescale):\n",
    "    results = pd.DataFrame(columns = [\"sku\",\"target\",\"prediction\"])\n",
    "\n",
    "    for sku in y_test[\"sku\"].unique():\n",
    "\n",
    "        pred_sku = prediction[np.where(y_test[\"sku\"] == sku)]\n",
    "        originals = rescale.loc[rescale[\"sku\"] == sku][\"target\"]\n",
    "        pred = unscale(pred_sku,originals)\n",
    "\n",
    "        results = pd.concat([results, pd.DataFrame({\n",
    "            \"date\" : y_test[y_test[\"sku\"] == sku][\"date\"],\n",
    "            \"sku\" : y_test[y_test[\"sku\"] == sku][\"sku\"],\n",
    "            \"scaled_target\" : y_test[y_test[\"sku\"] == sku][\"scaled_target\"],\n",
    "            \"target\" : y_test[y_test[\"sku\"] == sku][\"target\"],\n",
    "            \"scaled_prediction\" : pred_sku,\n",
    "            \"prediction\" : pred\n",
    "        })])\n",
    "    results = results.dropna().set_index(\"date\")\n",
    "    return mean_absolute_percentage_error(results[\"target\"],results[\"prediction\"])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_scaled_results(prediction, y_test, rescale):\n",
    "    results = pd.DataFrame(columns = [\"sku\",\"target\",\"prediction\"])\n",
    "\n",
    "    for sku in y_test[\"sku\"].unique():\n",
    "\n",
    "        pred_sku = prediction[np.where(y_test[\"sku\"] == sku)]\n",
    "        originals = rescale.loc[rescale[\"sku\"] == sku][\"target\"]\n",
    "        pred = unscale(pred_sku,originals)\n",
    "\n",
    "        results = pd.concat([results, pd.DataFrame({\n",
    "            \"date\" : y_test[y_test[\"sku\"] == sku][\"date\"],\n",
    "            \"sku\" : y_test[y_test[\"sku\"] == sku][\"sku\"],\n",
    "            \"scaled_target\" : y_test[y_test[\"sku\"] == sku][\"scaled_target\"],\n",
    "            \"target\" : y_test[y_test[\"sku\"] == sku][\"target\"],\n",
    "            \"scaled_prediction\" : pred_sku,\n",
    "            \"prediction\" : pred\n",
    "        })])\n",
    "    results = results.dropna().set_index(\"date\")\n",
    "\n",
    "    return plot_results(results[\"target\"], results[\"prediction\"], \"target\"), results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTION THAT COMPUTE THE CROSS PREDICTION OF THE TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_training(model,X,y,label = \"scaled_target\",span = 24):\n",
    "    \n",
    "    #k = number of folds\n",
    "    k = int(len(X)/span)\n",
    "    \n",
    "    prediction = pd.DataFrame(columns = [\"date\",\"sku\",\"pred\"])    \n",
    "    \n",
    "    for fold in range(0,k):\n",
    "        if fold != k - 1:\n",
    "            validation = X.iloc[fold*span:(fold+1)*span]\n",
    "        else:\n",
    "            validation = X.iloc[fold*span:]\n",
    "        \n",
    "        training = X[~X.index.isin(validation.index)]\n",
    "        y_tr  = y[~y.index.isin(validation.index)]\n",
    "        y_val  = y[y.index.isin(validation.index)]\n",
    "        \n",
    "        model.fit(training, y_tr[label])\n",
    "        partial_pred = pd.DataFrame()\n",
    "        partial_pred[\"date\"] = y_val[\"date\"]\n",
    "        partial_pred[\"sku\"] = y_val[\"sku\"]\n",
    "        partial_pred[\"pred\"] = model.predict(validation)\n",
    "        partial_pred.set_index(validation.index)\n",
    "        prediction = prediction.append(partial_pred)\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTION THAT COMPUTE THE PREDITION OF THE TEST SET STACKING ALL THE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackingPred(models,final_model, X_train, y_train, X_test,y_test,features_for_model,rescale = None, span = 24, label = \"scaled_target\"):\n",
    "    final_prediction = []\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    \n",
    "    y_traing = y_train\n",
    "    X_traing = X_train\n",
    "    sku_dummies = pd.get_dummies(y_traing[\"sku\"]) > 0\n",
    "    sku_dummies_df = pd.DataFrame(sku_dummies, index = X_traing.index)\n",
    "    X_traing = pd.concat([sku_dummies_df,X_traing],axis = 1)\n",
    "    \n",
    "    for week in y_test.date.unique():\n",
    "        print(week)\n",
    "        \n",
    "        test = X_test.loc[y_test.date == week]\n",
    "        y_te = y_test.loc[y_test.date == week]\n",
    "        \n",
    "        #TRAIN SET PREDICTION WITH MODELS\n",
    "        results_dict = {}\n",
    "        print(X_traing.dropna().shape)\n",
    "        for name, model in models.items():\n",
    "            \n",
    "            print(\"train with %s shape: %d x %d\" %(name,X_traing.shape[0],X_traing.shape[1]))            \n",
    "            results_dict.update({name : predict_training(model, X_traing, y_traing,label,span)})\n",
    "\n",
    "        train_result = pd.DataFrame(index = X_traing.index)\n",
    "        for name,r in results_dict.items():\n",
    "            train_result[\"date\"] = r[\"date\"]\n",
    "            train_result[\"sku\"] = r[\"sku\"]\n",
    "            train_result[name] = r[\"pred\"]\n",
    "        onehot_encoded = pd.get_dummies(y_te[\"sku\"]) > 0\n",
    "        onehot = pd.DataFrame(onehot_encoded, index = test.index)\n",
    "        test = pd.concat([onehot,test],axis = 1)\n",
    "\n",
    "        #TEST SET PREDICTION WITH MODELS\n",
    "        prediction_df = pd.DataFrame()\n",
    "        for name, model in models.items():\n",
    "            print(\"test with %s\" %name)\n",
    "          \n",
    "            model = model.fit(X_traing, y_traing[label])\n",
    "\n",
    "            prediction_df[name] = model.predict(test)\n",
    "        prediction_df.index = test.index\n",
    "\n",
    "        #DUMMY VARIABLES FOR FINAL TRAIN AND TEST\n",
    "        final_train = pd.concat([X_traing,train_result[models.keys()]],axis = 1)\n",
    "        final_test = pd.concat([test,prediction_df[models.keys()]],axis = 1)\n",
    "       # print(final_test)\n",
    "        #TRAIN ON TRAIN PREDICTIONS AND PREDICTION ON TEST PREDICTIONS\n",
    "        print(\"final prediction\")\n",
    "\n",
    "\n",
    "        final_model.fit(final_train[features_for_model[\"final\"]], y_traing[label])\n",
    "        p =  final_model.predict(final_test[features_for_model[\"final\"]])\n",
    "        final_prediction = np.append(final_prediction,p)\n",
    "        if rescale is not None:\n",
    "            print(\"MAPE finale %.5f\" % scaled_mape(p, y_te, rescale))\n",
    "            print(\"MAPE xgb %.5f\" % scaled_mape(np.array(prediction_df[\"xgb\"]), y_te, rescale))\n",
    "            print(\"MAPE extra %.5f\" % scaled_mape(np.array(prediction_df[\"extra\"]), y_te, rescale))\n",
    "        X_traing = pd.concat([X_traing,test]).reset_index().drop(columns=[\"index\"])\n",
    "        y_traing = pd.concat([y_traing,y_te]).reset_index().drop(columns=[\"index\"])\n",
    "    return final_prediction, train_result, prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET PROCESSED DATA FROM CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/processed_train.csv',index_col = 0)\n",
    "df_test = pd.read_csv('data/processed_test.csv',index_col = 0)\n",
    "\n",
    "df = df.loc[df.scope == 1]\n",
    "df_test = df_test.loc[df.scope == 1]\n",
    "df.date = df.date.apply(lambda x:datetime.strptime(x, '%Y-%m-%d'))\n",
    "df_test.date = df_test.date.apply(lambda x:datetime.strptime(x, '%Y-%m-%d'))\n",
    "\n",
    "df = df.sort_values(by=[\"date\",'sku'])\n",
    "df_test = df_test.sort_values(by=[\"date\",'sku'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale_df = pd.concat([df,df_test])[[\"target\",\"sku\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,df_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIRST FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_df = df[['scaled_quarter','scaled_month','scaled_year','scaled_price_diff1',\n",
    "            \"group_A\",\"group_B\",\"BRAND2\",\"BRAND4\",\n",
    "           'scaled_dayofyear','scaled_dayofmonth','scaled_weekofyear',\"scaled_sales1\",\"scaled_sales2\",\"scaled_sales3\",\n",
    "            'scaled_promo','scaled_diff1','scaled_diff2','percentage_diff1','scaled_price',\"scaled_rolling2\",\n",
    "                \"date\",\"sku\",\"scaled_target\",\"target\"]]\n",
    "simple_df = simple_df.dropna().set_index(\"date\")\n",
    "simple_train = simple_df[:SplitTestDate].reset_index()\n",
    "simple_test = simple_df[SplitTestDate:].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = simple_train[['scaled_month',\n",
    "            \"group_A\",\"group_B\",\"BRAND2\",\"BRAND4\",\n",
    "           'scaled_dayofyear','scaled_dayofmonth','scaled_weekofyear',\"scaled_sales1\",\"scaled_sales2\",\"scaled_sales3\",\n",
    "            'scaled_promo','scaled_diff1','scaled_diff2','percentage_diff1','scaled_price',\"scaled_rolling2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = simple_train[[\"date\",\"sku\",\"scaled_target\",\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = simple_test[X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = simple_test[y_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_train,X_test.loc[y_test.date == y_test.date.unique()[0]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_month</th>\n",
       "      <th>group_A</th>\n",
       "      <th>group_B</th>\n",
       "      <th>BRAND2</th>\n",
       "      <th>BRAND4</th>\n",
       "      <th>scaled_dayofyear</th>\n",
       "      <th>scaled_dayofmonth</th>\n",
       "      <th>scaled_weekofyear</th>\n",
       "      <th>scaled_sales1</th>\n",
       "      <th>scaled_sales2</th>\n",
       "      <th>scaled_sales3</th>\n",
       "      <th>scaled_promo</th>\n",
       "      <th>scaled_diff1</th>\n",
       "      <th>scaled_diff2</th>\n",
       "      <th>percentage_diff1</th>\n",
       "      <th>scaled_price</th>\n",
       "      <th>scaled_rolling2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407090</td>\n",
       "      <td>0.486490</td>\n",
       "      <td>0.359256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>-0.047834</td>\n",
       "      <td>0.165436</td>\n",
       "      <td>0.611354</td>\n",
       "      <td>0.446790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.090556</td>\n",
       "      <td>0.109741</td>\n",
       "      <td>0.138842</td>\n",
       "      <td>0.119108</td>\n",
       "      <td>0.019184</td>\n",
       "      <td>0.048286</td>\n",
       "      <td>0.064815</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.100149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.106018</td>\n",
       "      <td>0.102699</td>\n",
       "      <td>0.131821</td>\n",
       "      <td>0.203441</td>\n",
       "      <td>-0.003319</td>\n",
       "      <td>0.025803</td>\n",
       "      <td>-0.016304</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.104359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.098738</td>\n",
       "      <td>0.121289</td>\n",
       "      <td>0.154270</td>\n",
       "      <td>0.207252</td>\n",
       "      <td>0.022552</td>\n",
       "      <td>0.055532</td>\n",
       "      <td>0.068904</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.110013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.322606</td>\n",
       "      <td>0.418289</td>\n",
       "      <td>0.333547</td>\n",
       "      <td>0.990468</td>\n",
       "      <td>0.095683</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.246065</td>\n",
       "      <td>0.579439</td>\n",
       "      <td>0.370447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.484765</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.142010</td>\n",
       "      <td>0.192205</td>\n",
       "      <td>0.182352</td>\n",
       "      <td>0.077256</td>\n",
       "      <td>0.050195</td>\n",
       "      <td>0.040342</td>\n",
       "      <td>0.166026</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.167107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.484765</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.116144</td>\n",
       "      <td>0.104440</td>\n",
       "      <td>0.048294</td>\n",
       "      <td>0.067316</td>\n",
       "      <td>0.055612</td>\n",
       "      <td>0.484927</td>\n",
       "      <td>0.677570</td>\n",
       "      <td>0.082486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.484765</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.212171</td>\n",
       "      <td>0.254172</td>\n",
       "      <td>0.240136</td>\n",
       "      <td>0.226122</td>\n",
       "      <td>0.042001</td>\n",
       "      <td>0.027965</td>\n",
       "      <td>0.116567</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.233171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.484765</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.221990</td>\n",
       "      <td>0.253979</td>\n",
       "      <td>0.255097</td>\n",
       "      <td>0.198803</td>\n",
       "      <td>0.031988</td>\n",
       "      <td>0.033107</td>\n",
       "      <td>0.069733</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.237985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.484765</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.158397</td>\n",
       "      <td>0.208994</td>\n",
       "      <td>0.211487</td>\n",
       "      <td>0.102199</td>\n",
       "      <td>0.050597</td>\n",
       "      <td>0.053090</td>\n",
       "      <td>0.126261</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.183695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1572 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      scaled_month  group_A  group_B  BRAND2  BRAND4  scaled_dayofyear  \\\n",
       "0         1.000000    False     True    True   False          1.000000   \n",
       "1         1.000000     True    False    True   False          1.000000   \n",
       "2         1.000000     True    False    True   False          1.000000   \n",
       "3         1.000000     True    False    True   False          1.000000   \n",
       "4         1.000000    False     True   False    True          1.000000   \n",
       "...            ...      ...      ...     ...     ...               ...   \n",
       "1567      0.454545     True    False    True   False          0.484765   \n",
       "1568      0.454545    False     True   False    True          0.484765   \n",
       "1569      0.454545     True    False   False    True          0.484765   \n",
       "1570      0.454545     True    False   False    True          0.484765   \n",
       "1571      0.454545     True    False   False    True          0.484765   \n",
       "\n",
       "      scaled_dayofmonth  scaled_weekofyear  scaled_sales1  scaled_sales2  \\\n",
       "0              1.000000           1.000000       0.407090       0.486490   \n",
       "1              1.000000           1.000000       0.090556       0.109741   \n",
       "2              1.000000           1.000000       0.106018       0.102699   \n",
       "3              1.000000           1.000000       0.098738       0.121289   \n",
       "4              1.000000           1.000000       0.322606       0.418289   \n",
       "...                 ...                ...            ...            ...   \n",
       "1567           0.933333           0.490196       0.142010       0.192205   \n",
       "1568           0.933333           0.490196       0.048828       0.116144   \n",
       "1569           0.933333           0.490196       0.212171       0.254172   \n",
       "1570           0.933333           0.490196       0.221990       0.253979   \n",
       "1571           0.933333           0.490196       0.158397       0.208994   \n",
       "\n",
       "      scaled_sales3  scaled_promo  scaled_diff1  scaled_diff2  \\\n",
       "0          0.359256      1.000000      0.079400     -0.047834   \n",
       "1          0.138842      0.119108      0.019184      0.048286   \n",
       "2          0.131821      0.203441     -0.003319      0.025803   \n",
       "3          0.154270      0.207252      0.022552      0.055532   \n",
       "4          0.333547      0.990468      0.095683      0.010941   \n",
       "...             ...           ...           ...           ...   \n",
       "1567       0.182352      0.077256      0.050195      0.040342   \n",
       "1568       0.104440      0.048294      0.067316      0.055612   \n",
       "1569       0.240136      0.226122      0.042001      0.027965   \n",
       "1570       0.255097      0.198803      0.031988      0.033107   \n",
       "1571       0.211487      0.102199      0.050597      0.053090   \n",
       "\n",
       "      percentage_diff1  scaled_price  scaled_rolling2  \n",
       "0             0.165436      0.611354         0.446790  \n",
       "1             0.064815      0.633333         0.100149  \n",
       "2            -0.016304      0.777778         0.104359  \n",
       "3             0.068904      0.633333         0.110013  \n",
       "4             0.246065      0.579439         0.370447  \n",
       "...                ...           ...              ...  \n",
       "1567          0.166026      0.961538         0.167107  \n",
       "1568          0.484927      0.677570         0.082486  \n",
       "1569          0.116567      0.961538         0.233171  \n",
       "1570          0.069733      0.718750         0.237985  \n",
       "1571          0.126261      0.727273         0.183695  \n",
       "\n",
       "[1572 rows x 17 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.reset_index().drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPUTE THE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29T00:00:00.000000000\n",
      "(1560, 29)\n",
      "train with xgb shape: 1560 x 29\n",
      "train with extra shape: 1560 x 29\n",
      "test with xgb\n",
      "test with extra\n",
      "final prediction\n",
      "MAPE finale 9.77505\n",
      "MAPE xgb 10.10316\n",
      "MAPE extra 10.06821\n",
      "2019-07-06T00:00:00.000000000\n",
      "(1572, 29)\n",
      "train with xgb shape: 1572 x 29\n",
      "train with extra shape: 1572 x 29\n"
     ]
    }
   ],
   "source": [
    "res = StackingPred(models,finalreg, X_train, y_train, X_test,y_test,features_for_model,rescale = rescale_df, span = 36, label = \"scaled_target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHOW THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape,results = plot_scaled_results(res, y_test, rescale_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

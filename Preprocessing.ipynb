{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/raw/train.csv')\n",
    "test_df = pd.read_csv('../data/raw/x_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2689, 1027, 2696, 2698, 1035,  144, 2704, 1554, 2705, 2707, 2711,\n",
       "       2712, 1051, 2718,  546, 1058,  549, 1065,  554,  686,  688, 1206,\n",
       "       2360, 2365, 1472, 1600, 1603, 1732, 1608, 2249, 1356, 1618, 1365,\n",
       "       1371, 2396, 1633, 2401, 2410, 1516, 2678, 2681, 2682, 2683])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sku.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1027, 1035,  144, 1051,  546, 1058,  549, 1065,  554,  686,  688,\n",
       "       1206])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sku.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop 2689 becaouse it is not correlated to anything\n",
    "train_df = train_df[train_df.sku != 2689]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.rename({\"Unnamed: 0\":\"date\"}, axis=1)\n",
    "train_df.date = train_df.date.apply(lambda x:datetime.strptime(x[3:], '%d %B %Y'))\n",
    "test_df = test_df.rename({\"Unnamed: 0\":\"date\"}, axis=1)\n",
    "test_df.date = test_df.date.apply(lambda x:datetime.strptime(x[3:], '%d %B %Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5544 entries, 134 to 5718\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   date                 5544 non-null   datetime64[ns]\n",
      " 1   sku                  5544 non-null   int64         \n",
      " 2   pack                 5544 non-null   object        \n",
      " 3   size (GM)            5544 non-null   float64       \n",
      " 4   brand                5544 non-null   object        \n",
      " 5   price                5544 non-null   float64       \n",
      " 6   POS_exposed w-1      5544 non-null   float64       \n",
      " 7   volume_on_promo w-1  5544 non-null   float64       \n",
      " 8   sales w-1            5544 non-null   float64       \n",
      " 9   scope                5544 non-null   int64         \n",
      " 10  target               5544 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(6), int64(2), object(2)\n",
      "memory usage: 519.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# on train dataframe values of \"2016-12-10\" raw regarding the week -1 are null so we drop them\n",
    "train_df = train_df.dropna()\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisogna decidere se lasciare l'ultima riga del test con target nan (perchè andrà predetto per la consegna) o se togliere la riga (perchè non si riesce a calcolarci il mape non avendo il valore reale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make of target column on test set\n",
    "temp = pd.DataFrame(columns = train_df.columns)\n",
    "\n",
    "for sku in test_df['sku'].unique():\n",
    "    sales_sku = test_df.loc[test_df['sku'] == sku]\n",
    "    #nan on the last row becouse we don't have the value\n",
    "    sales_sku['target'] = sales_sku[\"sales w-1\"].shift(-1)\n",
    "    temp = pd.concat((temp, sales_sku), axis = 0)\n",
    "\n",
    "test_df = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_transformer = preprocessing.QuantileTransformer(output_distribution='normal', random_state=123)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "def scale(feature, scaler = min_max_scaler):\n",
    "    size = len(feature)\n",
    "    return scaler.fit_transform(np.array([feature]).reshape(size, 1)).T[0]\n",
    "def unscale(scaled, original, scaler = min_max_scaler):\n",
    "    size2 = len(scaled)\n",
    "    size1 = len(original)\n",
    "    return scaler.fit(np.array([original]).reshape(size1, 1)).inverse_transform(np.array([scaled]).reshape(size2, 1)).T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \n",
    "    df2 = pd.DataFrame(columns= df.columns)\n",
    "    sku_groups = {\n",
    "        \"A\" :[1603, 2705, 1608, 1618, 1371],\n",
    "        \"B\" :[2249, 2401, 2365, 2410],\n",
    "        \"C\" :[1554, 1600],\n",
    "        \"D\" :[2678, 2683],\n",
    "        \"E\" :[1365, 1732, 1472, 2682],\n",
    "        \"F\" :[144, 686, 1051],\n",
    "        \"G\" :[688,1058,549,546,1027,1035,554,1206,1065,1365,1732,1472,1600,1554,1516,1633,1371\n",
    "              ,1356,2678,2683,2681,1603,2705,1618,2365,2696,2718,2711,2712,2698,2396,2682,2707,2704,2360]         \n",
    "    }\n",
    "    \n",
    "\n",
    "    for sku in df['sku'].unique():\n",
    "        sales_sku = df.loc[df['sku'] == sku]\n",
    "        \n",
    "        for l,g in sku_groups.items():\n",
    "            if sku in g:\n",
    "                if \"corr_group\" in sales_sku.columns:\n",
    "                    sales_sku[\"corr_group\"] = sales_sku[\"corr_group\"].apply(lambda x: x + \" \" + l)\n",
    "                else:\n",
    "                    sales_sku[\"corr_group\"] = np.full((len(sales_sku[\"price\"]),), l)\n",
    "              \n",
    "        sales_sku['sales w-2'] = sales_sku['sales w-1'].shift(1)\n",
    "        sales_sku['sales w-3'] = sales_sku['sales w-1'].shift(2)\n",
    "                    \n",
    "        sales_sku[\"rolling1\"] = sales_sku[\"sales w-1\"].rolling(1).mean()\n",
    "        sales_sku[\"rolling2\"] = sales_sku[\"sales w-1\"].rolling(2).mean()\n",
    "        sales_sku[\"rolling3\"] = sales_sku[\"sales w-1\"].rolling(3).mean()\n",
    "        sales_sku[\"rolling4\"] = sales_sku[\"sales w-1\"].rolling(4).mean()\n",
    "        sales_sku[\"rolling5\"] = sales_sku[\"sales w-1\"].rolling(5).mean()\n",
    "\n",
    "        sales_sku['diff1'] = -(sales_sku['sales w-1'] - sales_sku['sales w-1'].shift(1))\n",
    "        sales_sku['diff2'] = -(sales_sku['sales w-1'] - sales_sku['sales w-1'].shift(2))\n",
    "        sales_sku['diff3'] = -(sales_sku['sales w-1'] - sales_sku['sales w-1'].shift(3))\n",
    " \n",
    "        sales_sku[\"scaled_target\"] = scale(sales_sku[\"target\"])\n",
    "        sales_sku[\"scaled_price\"] = scale(sales_sku[\"price\"])\n",
    "        sales_sku['scaled_sales1'] = scale(sales_sku['sales w-1'])\n",
    "        sales_sku[\"scaled_promo\"] = scale(sales_sku[\"volume_on_promo w-1\"])\n",
    "        \n",
    "        sales_sku[\"scaled_rolling1\"] = sales_sku[\"scaled_sales1\"].rolling(1).mean()\n",
    "        sales_sku[\"scaled_rolling2\"] = sales_sku[\"scaled_sales1\"].rolling(2).mean()\n",
    "        sales_sku[\"scaled_rolling3\"] = sales_sku[\"scaled_sales1\"].rolling(3).mean()\n",
    "        sales_sku[\"scaled_rolling4\"] = sales_sku[\"scaled_sales1\"].rolling(4).mean()\n",
    "        sales_sku[\"scaled_rolling5\"] = sales_sku[\"scaled_sales1\"].rolling(5).mean()\n",
    "        \n",
    "        sales_sku['scaled_diff1'] = -(sales_sku['scaled_sales1'] - sales_sku['scaled_sales1'].shift(1))\n",
    "        sales_sku['scaled_diff2'] = -(sales_sku['scaled_sales1'] - sales_sku['scaled_sales1'].shift(2))\n",
    "        sales_sku['scaled_diff3'] = -(sales_sku['scaled_sales1'] - sales_sku['scaled_sales1'].shift(3))\n",
    "        sales_sku['percentage_diff1'] = -(sales_sku['sales w-1'] - sales_sku['sales w-1'].shift(1))/sales_sku['sales w-1']\n",
    "        sales_sku['scaled_price_diff1'] = -(sales_sku['scaled_price'] - sales_sku['scaled_price'].shift(1))\n",
    "      \n",
    "        df2 = pd.concat((df2, sales_sku), axis = 0)\n",
    "    \n",
    "    #df2 = df2.dropna()\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    onehot_encoded = df2[\"corr_group\"].str.get_dummies(\" \") > 0\n",
    "    onehot = pd.DataFrame(onehot_encoded, index = df2.index)\n",
    "    \n",
    "    onehot_encoded = df2[\"brand\"].str.get_dummies() > 0\n",
    "    onehot2 = pd.DataFrame(onehot_encoded, index = df2.index)\n",
    "\n",
    "    df = pd.concat([df2, onehot, onehot2], axis=1, sort=False)\n",
    "    \n",
    "    df = df.drop(columns=[\"pack\",\"size (GM)\",\"brand\",\"corr_group\",\"POS_exposed w-1\"])\n",
    "    \n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['dayofmonth'] = df['date'].dt.day\n",
    "    df['weekofyear'] = df['date'].dt.weekofyear\n",
    "    \n",
    "    df['scaled_quarter'] = scale(df['quarter'])\n",
    "    df['scaled_month'] = scale(df['month'])\n",
    "    df['scaled_year'] = scale(df['year'])\n",
    "    df['scaled_dayofyear'] = scale(df['dayofyear'])\n",
    "    df['scaled_dayofmonth'] = scale(df['dayofmonth'])\n",
    "    df['scaled_weekofyear'] = scale(df['weekofyear'])\n",
    "    \n",
    "    df[\"sku\"] = df[\"sku\"].apply(lambda x: int(x))\n",
    "    \n",
    "    cols = list(df.columns.values)\n",
    "    cols.pop(cols.index('target'))\n",
    "    cols.pop(cols.index('scaled_target'))\n",
    "    df = df[cols+['scaled_target','target']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"scope\"] = train_df.scope.apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_scope0 = train_df.loc[train_df.scope == 0]\n",
    "train_df_scope1 = train_df.loc[train_df.scope == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df1 = pd.concat([train_df_scope1,test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_scope0 = create_features(train_df_scope0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df1 = create_features(all_df1)\n",
    "all_df1 = all_df1.set_index(\"date\")\n",
    "processed_train_scope1 = all_df1[:\"2019-06-22\"].reset_index()\n",
    "processed_test = all_df1[\"2019-06-23\":].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5544 entries, 267 to 1583\n",
      "Data columns (total 55 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   date                 5544 non-null   datetime64[ns]\n",
      " 1   sku                  5544 non-null   int64         \n",
      " 2   price                5544 non-null   float64       \n",
      " 3   volume_on_promo w-1  5544 non-null   float64       \n",
      " 4   sales w-1            5544 non-null   float64       \n",
      " 5   scope                5544 non-null   object        \n",
      " 6   sales w-2            5502 non-null   float64       \n",
      " 7   sales w-3            5460 non-null   float64       \n",
      " 8   rolling1             5544 non-null   float64       \n",
      " 9   rolling2             5502 non-null   float64       \n",
      " 10  rolling3             5460 non-null   float64       \n",
      " 11  rolling4             5418 non-null   float64       \n",
      " 12  rolling5             5376 non-null   float64       \n",
      " 13  diff1                5502 non-null   float64       \n",
      " 14  diff2                5460 non-null   float64       \n",
      " 15  diff3                5418 non-null   float64       \n",
      " 16  scaled_price         5544 non-null   float64       \n",
      " 17  scaled_sales1        5544 non-null   float64       \n",
      " 18  scaled_promo         5544 non-null   float64       \n",
      " 19  scaled_rolling1      5544 non-null   float64       \n",
      " 20  scaled_rolling2      5502 non-null   float64       \n",
      " 21  scaled_rolling3      5460 non-null   float64       \n",
      " 22  scaled_rolling4      5418 non-null   float64       \n",
      " 23  scaled_rolling5      5376 non-null   float64       \n",
      " 24  scaled_diff1         5502 non-null   float64       \n",
      " 25  scaled_diff2         5460 non-null   float64       \n",
      " 26  scaled_diff3         5418 non-null   float64       \n",
      " 27  percentage_diff1     5502 non-null   float64       \n",
      " 28  scaled_price_diff1   5502 non-null   float64       \n",
      " 29  A                    3960 non-null   object        \n",
      " 30  B                    3960 non-null   object        \n",
      " 31  C                    3960 non-null   object        \n",
      " 32  D                    3960 non-null   object        \n",
      " 33  E                    3960 non-null   object        \n",
      " 34  G                    5544 non-null   bool          \n",
      " 35  BRAND1               3960 non-null   object        \n",
      " 36  BRAND3               3960 non-null   object        \n",
      " 37  BRAND5               3960 non-null   object        \n",
      " 38  quarter              5544 non-null   int64         \n",
      " 39  month                5544 non-null   int64         \n",
      " 40  year                 5544 non-null   int64         \n",
      " 41  dayofyear            5544 non-null   int64         \n",
      " 42  dayofmonth           5544 non-null   int64         \n",
      " 43  weekofyear           5544 non-null   int64         \n",
      " 44  scaled_quarter       5544 non-null   float64       \n",
      " 45  scaled_month         5544 non-null   float64       \n",
      " 46  scaled_year          5544 non-null   float64       \n",
      " 47  scaled_dayofyear     5544 non-null   float64       \n",
      " 48  scaled_dayofmonth    5544 non-null   float64       \n",
      " 49  scaled_weekofyear    5544 non-null   float64       \n",
      " 50  scaled_target        5544 non-null   float64       \n",
      " 51  target               5544 non-null   float64       \n",
      " 52  F                    1584 non-null   object        \n",
      " 53  BRAND2               1584 non-null   object        \n",
      " 54  BRAND4               1584 non-null   object        \n",
      "dtypes: bool(1), datetime64[ns](1), float64(34), int64(7), object(12)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "processed_train = pd.concat([processed_train_scope0,processed_train_scope1])\n",
    "processed_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train[\"BRAND1\"] = processed_train[\"BRAND1\"].fillna(False)\n",
    "processed_train[\"BRAND2\"] = processed_train[\"BRAND2\"].fillna(False)\n",
    "processed_train[\"BRAND3\"] = processed_train[\"BRAND3\"].fillna(False)\n",
    "processed_train[\"BRAND4\"] = processed_train[\"BRAND4\"].fillna(False)\n",
    "processed_train[\"BRAND5\"] = processed_train[\"BRAND5\"].fillna(False)\n",
    "processed_train[\"A\"] = processed_train[\"A\"].fillna(False)\n",
    "processed_train[\"B\"] = processed_train[\"B\"].fillna(False)\n",
    "processed_train[\"C\"] = processed_train[\"C\"].fillna(False)\n",
    "processed_train[\"D\"] = processed_train[\"D\"].fillna(False)\n",
    "processed_train[\"E\"] = processed_train[\"E\"].fillna(False)\n",
    "processed_train[\"F\"] = processed_train[\"F\"].fillna(False)\n",
    "processed_train[\"G\"] = processed_train[\"G\"].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train.to_csv(\"processed_train.csv\")\n",
    "processed_test.to_csv(\"processed_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
